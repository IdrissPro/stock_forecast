{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2XWBXy5AzNd"
      },
      "source": [
        "# Modèle GINN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5TpASGYAzNe"
      },
      "source": [
        "Dans ce notebook, on s'intéresse à l'amélioration éventuelle des performances d'un LSTM classique à l'aide du modèle GINN présenté dans le papier: https://arxiv.org/pdf/2410.00288 \\\n",
        "Nous utilisons ici également les rendements logarithmiques. \\\n",
        "L'architecture GINN que nous avons choisie est similaire à celle du papier:\n",
        "un modèle GARCH (1,1) avec moyenne constante qui nous fournit des prédictions de volatilité et de moyenne \\\n",
        "un calcul de la \"vraie\" volatilité à partir de la moyenne calculée par le modèle GARCH \\\n",
        "un modèle LSTM qui effectue des prédictions de volatilité à partir des \"vraies\" volatilités et de la volatilité prédite par le GARCH en utilisant une fonction de coût customisée\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkDUOQyYjcwo",
        "outputId": "a9de0a07-66ec-4972-b429-bbde9ca2619b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arch\n",
            "  Downloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from arch) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from arch) (1.13.1)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.11/dist-packages (from arch) (2.2.2)\n",
            "Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.11/dist-packages (from arch) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2025.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.12->arch) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.12->arch) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->arch) (1.17.0)\n",
            "Downloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (985 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m985.3/985.3 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: arch\n",
            "Successfully installed arch-7.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install arch\n",
        "from arch import arch_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjH5zqCza5Je",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35218301-7b2b-4be9-a03b-67c806116731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.52)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras import layers, models,Input,regularizers\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nE865CEAzNh"
      },
      "source": [
        "## Chargement des données et division train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWeNiWt1a5Jg"
      },
      "outputs": [],
      "source": [
        "def load_data(symbol=\"SPY\", start=\"1994-01-01\", end=\"2021-01-01\", interval='1d'): #1 donnée=1jour\n",
        "    df = yf.download(symbol, start=start, end=end, interval=interval)\n",
        "    df.reset_index(inplace=True)\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df = df[['Date', 'Close']]\n",
        "    df.dropna(inplace=True)\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df.set_index('Date', inplace=True)\n",
        "    return df\n",
        "\n",
        "def train_test(list_df, train_size=0.7, test_size=0.3):\n",
        "    result = []\n",
        "    for df in list_df:\n",
        "        X = df.index.values.reshape(-1, 1)\n",
        "        y = df['Close'].values\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, train_size=train_size, test_size=test_size, shuffle=False\n",
        "        )\n",
        "        result.append([X_train, X_test, y_train, y_test])\n",
        "    return result\n",
        "\n",
        "def etl_pipeline(train_size=0.7, test_size=0.3):\n",
        "    # Chargement\n",
        "    gspc = load_data('^GSPC')\n",
        "    dji = load_data('^DJI')\n",
        "    nyse = load_data('^NYA')\n",
        "\n",
        "    # Preprocess\n",
        "    gspc_close = preprocess_data(gspc)\n",
        "    dji_close = preprocess_data(dji)\n",
        "    nyse_close = preprocess_data(nyse)\n",
        "\n",
        "    list_df_close = [gspc_close, dji_close, nyse_close]\n",
        "\n",
        "    # Pour faire le split et shift la data de 90 jours\n",
        "    def train_test_shift(df, train_size, shift_days=90):\n",
        "        # Split\n",
        "        train_size = int(len(df) * train_size)\n",
        "        X_train = df.iloc[:train_size]\n",
        "        X_test = df.iloc[train_size:]\n",
        "\n",
        "        # Shift\n",
        "        y_train = df.iloc[shift_days:train_size + shift_days]\n",
        "        y_test = df.iloc[train_size + shift_days:]\n",
        "\n",
        "        X_train = X_train.iloc[:-shift_days]\n",
        "        X_test = X_test.iloc[:-shift_days]\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    # Appliquer train_test_shift à tous les dataframes\n",
        "    splits = [train_test_shift(df, train_size) for df in list_df_close]\n",
        "\n",
        "    return {\n",
        "        \"GSPC\": {\"X_train\": splits[0][0], \"X_test\": splits[0][1], \"y_train\": splits[0][2], \"y_test\": splits[0][3]},\n",
        "        \"DJI\":  {\"X_train\": splits[1][0], \"X_test\": splits[1][1], \"y_train\": splits[1][2], \"y_test\": splits[1][3]},\n",
        "        \"NYSE\": {\"X_train\": splits[2][0], \"X_test\": splits[2][1], \"y_train\": splits[2][2], \"y_test\": splits[2][3]},\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXm6YWhIAzNj"
      },
      "source": [
        "Nous avons choisi 90 jours de shift comme dans l'article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG_lL4l2a5Jh",
        "outputId": "ba652f71-ef1f-46ad-8870-6340a7c36423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "gspc=load_data('^GSPC')\n",
        "dji=load_data('^DJI')\n",
        "nyse=load_data('^NYA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46n6Z86va5Ji",
        "outputId": "ade74420-c818-4221-cdd9-38012e484416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-47841b545f8f>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.dropna(inplace=True)\n",
            "<ipython-input-3-47841b545f8f>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n",
            "<ipython-input-3-47841b545f8f>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.dropna(inplace=True)\n",
            "<ipython-input-3-47841b545f8f>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n",
            "<ipython-input-3-47841b545f8f>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.dropna(inplace=True)\n",
            "<ipython-input-3-47841b545f8f>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n"
          ]
        }
      ],
      "source": [
        "gspc_close=preprocess_data(gspc)\n",
        "dji_close=preprocess_data(dji)\n",
        "nyse_close=preprocess_data(nyse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuD5Zwtqa5Jj",
        "outputId": "0d4619cd-f5d2-4af2-b96d-07f3288d422b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Price             Close\n",
            "Ticker            ^GSPC\n",
            "Date                   \n",
            "1994-01-03   465.440002\n",
            "1994-01-04   466.890015\n",
            "1994-01-05   467.549988\n",
            "1994-01-06   467.119995\n",
            "1994-01-07   469.899994\n",
            "...                 ...\n",
            "2020-12-24  3703.060059\n",
            "2020-12-28  3735.360107\n",
            "2020-12-29  3727.040039\n",
            "2020-12-30  3732.040039\n",
            "2020-12-31  3756.070068\n",
            "\n",
            "[6799 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "print(gspc_close)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxRKgBdda5Jk",
        "outputId": "42fad116-4d21-4f7b-e731-518d54c55478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "<ipython-input-3-47841b545f8f>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.dropna(inplace=True)\n",
            "<ipython-input-3-47841b545f8f>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n",
            "<ipython-input-3-47841b545f8f>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.dropna(inplace=True)\n",
            "<ipython-input-3-47841b545f8f>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n",
            "<ipython-input-3-47841b545f8f>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.dropna(inplace=True)\n",
            "<ipython-input-3-47841b545f8f>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "result = etl_pipeline()\n",
        "\n",
        "# pour GSPC\n",
        "X_gspc_train = result[\"GSPC\"][\"X_train\"]\n",
        "X_gspc_test  = result[\"GSPC\"][\"X_test\"]\n",
        "y_gspc_train = result[\"GSPC\"][\"y_train\"]\n",
        "y_gspc_test  = result[\"GSPC\"][\"y_test\"]\n",
        "\n",
        "# pour DJI\n",
        "X_dji_train = result[\"DJI\"][\"X_train\"]\n",
        "X_dji_test  = result[\"DJI\"][\"X_test\"]\n",
        "y_dji_train = result[\"DJI\"][\"y_train\"]\n",
        "y_dji_test  = result[\"DJI\"][\"y_test\"]\n",
        "\n",
        "# pour NYSE\n",
        "X_nyse_train = result[\"NYSE\"][\"X_train\"]\n",
        "X_nyse_test  = result[\"NYSE\"][\"X_test\"]\n",
        "y_nyse_train = result[\"NYSE\"][\"y_train\"]\n",
        "y_nyse_test  = result[\"NYSE\"][\"y_test\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds0P4PDXq9I9",
        "outputId": "eb878f5f-6c6d-4944-e86d-fd61fd6e34e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Price              Close\n",
            "Ticker              ^DJI\n",
            "Date                    \n",
            "1994-01-03   3756.600098\n",
            "1994-01-04   3783.899902\n",
            "1994-01-05   3798.820068\n",
            "1994-01-06   3803.879883\n",
            "1994-01-07   3820.770020\n",
            "...                  ...\n",
            "2012-07-10  12653.120117\n",
            "2012-07-11  12604.530273\n",
            "2012-07-12  12573.269531\n",
            "2012-07-13  12777.089844\n",
            "2012-07-16  12727.209961\n",
            "\n",
            "[4669 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "print(X_dji_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeXGQc_ka5Jl",
        "outputId": "da4bce5e-1108-4a84-9b26-2ccfda836963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Price              Close\n",
            "Ticker              ^DJI\n",
            "Date                    \n",
            "1994-05-12   3652.840088\n",
            "1994-05-13   3659.679932\n",
            "1994-05-16   3671.500000\n",
            "1994-05-17   3720.610107\n",
            "1994-05-18   3732.889893\n",
            "...                  ...\n",
            "2013-04-01  14572.849609\n",
            "2013-04-02  14662.009766\n",
            "2013-04-03  14550.349609\n",
            "2013-04-04  14606.110352\n",
            "2013-04-05  14565.250000\n",
            "\n",
            "[4759 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "print(y_dji_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modèle GARCH\n",
        "Nous implémentons ici le modèle garch (1,1) à moyenne constante après avoir calculé les log returns de notre série temporelle\n"
      ],
      "metadata": {
        "id": "O0FwDaryYkXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTTfJ7KDAzNm"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "from arch import arch_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# log returns\n",
        "def calculate_log_returns(prices):\n",
        "    return np.log(prices / prices.shift(1)).fillna(0)\n",
        "\n",
        "# GARCH predictions\n",
        "def garch_predictions(prices_series, forecast_horizon=1):\n",
        "    mean_predictions = []\n",
        "    volatility_predictions = []\n",
        "    prediction_dates = []\n",
        "    true_volatility = []\n",
        "\n",
        "    prices_series = prices_series.sort_index()\n",
        "\n",
        "    # Calcul des log returns\n",
        "    log_returns = calculate_log_returns(prices_series)\n",
        "\n",
        "    # Au cas où il y ait un pb de données\n",
        "    if np.all(log_returns == log_returns.iloc[0]):\n",
        "        raise ValueError(\"Log returns are constant. Check the input data.\")\n",
        "\n",
        "    # ADF test pour checker la stationnarité de la série (hypothèse dans le cadre du modèle garch)\n",
        "    adf_result = adfuller(log_returns.dropna())\n",
        "    print(f\"ADF Statistic: {adf_result[0]}\")\n",
        "    print(f\"p-value: {adf_result[1]}\")\n",
        "\n",
        "    if adf_result[1] > 0.05:\n",
        "        print(\"Warning: Log returns may not be stationary.\")\n",
        "    else:\n",
        "        print(\"Log returns are not stationnary at the confidence level 0.05\")\n",
        "\n",
        "    for i in range(90, len(prices_series)):\n",
        "        window_prices = prices_series.iloc[i-90:i]\n",
        "        window_log_returns = calculate_log_returns(window_prices)\n",
        "\n",
        "        # Fitting\n",
        "        model = arch_model(window_log_returns, mean='constant', vol='GARCH', p=1, q=1, rescale=True) #Le rescaling des données est fortement recommandé pour améliorer les perfs du modèle\n",
        "        options = {\n",
        "            'maxiter': 1000,  # Nombre max d'itérations\n",
        "            'ftol': 1e-6,     # Tolérance  pour convergence\n",
        "        }\n",
        "        model_fit = model.fit(disp='off',options=options)\n",
        "\n",
        "        # Forecast mean et variance\n",
        "        forecast = model_fit.forecast(horizon=forecast_horizon)\n",
        "        predicted_mean = forecast.mean.iloc[-1].values[0]\n",
        "        predicted_volatility = forecast.variance.iloc[-1].values[0]\n",
        "\n",
        "        mean_predictions.append(predicted_mean)\n",
        "        volatility_predictions.append(predicted_volatility)\n",
        "        prediction_dates.append(prices_series.index[i])\n",
        "\n",
        "        #Calcul de \"true\" volatility\n",
        "        current_log_return = np.log(prices_series.iloc[i] / prices_series.iloc[i-1])\n",
        "        true_volatility.append((current_log_return - predicted_mean)**2 )\n",
        "\n",
        "    # Sortie:\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Date': prediction_dates,\n",
        "        'Predicted_Mean': mean_predictions,\n",
        "        'Predicted_Volatility': volatility_predictions,\n",
        "        'True_Volatility': true_volatility\n",
        "    }).set_index('Date')\n",
        "\n",
        "    return predictions_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Architecture du LSTM\n",
        "L'architecture implémentée est celle décrite dans l'article"
      ],
      "metadata": {
        "id": "FF0Ws-1sY7eV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGZTc-WOa5Jn"
      },
      "outputs": [],
      "source": [
        "#Modèle LSTM\n",
        "def create_lstm_model():\n",
        "    model = models.Sequential([\n",
        "        Input(shape=(90, 1)),  # Define input shape here\n",
        "        layers.LSTM(256, return_sequences=True),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.LSTM(256, return_sequences=True),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.LSTM(256),\n",
        "        layers.Dense(128),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Dense(1, kernel_regularizer=regularizers.l2(1e-4))\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exécution du modèle garch sur notre train test"
      ],
      "metadata": {
        "id": "_5s5fQBTZVTX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgxy5b3ja5Jo",
        "outputId": "798484bb-ee9e-452f-b9a8-3271d051cf99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADF Statistic: -15.829291476493498\n",
            "p-value: 9.987269738330167e-29\n",
            "[1.14372335e-04 8.02101321e-05 2.12485295e-04 ... 6.80310767e-04\n",
            " 3.71922090e-04 2.16250707e-05]\n",
            "[0.56489893 0.54862113 0.49840484 ... 0.89146804 0.94036626 0.98801149]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Calcul parallèle pour accélérer\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
        "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"8\"\n",
        "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"8\"\n",
        "\n",
        "tf.config.threading.set_intra_op_parallelism_threads(8)\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)\n",
        "\n",
        "prices_series = X_dji_train.iloc[:,0]\n",
        "predictions_df = garch_predictions(prices_series)\n",
        "\n",
        "true_volatility = predictions_df['True_Volatility'].values\n",
        "predicted_volatility = predictions_df['Predicted_Volatility'].values\n",
        "\n",
        "print(true_volatility)\n",
        "print(predicted_volatility)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSaADte3OwZO",
        "outputId": "ddf19684-d87e-44f9-87f7-790cad92234d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Predicted_Mean  Predicted_Volatility  True_Volatility\n",
            "Date                                                             \n",
            "1994-05-12       -0.004158              0.564899         0.000114\n",
            "1994-05-13       -0.007085              0.548621         0.000080\n",
            "1994-05-16       -0.011352              0.498405         0.000212\n",
            "1994-05-17       -0.009317              0.474618         0.000511\n",
            "1994-05-18       -0.002033              0.629019         0.000028\n",
            "...                    ...                   ...              ...\n",
            "2012-07-10       -0.020004              0.945773         0.000181\n",
            "2012-07-11       -0.025942              0.922671         0.000488\n",
            "2012-07-12       -0.028566              0.891468         0.000680\n",
            "2012-07-13       -0.003205              0.940366         0.000372\n",
            "2012-07-16        0.000739              0.988011         0.000022\n",
            "\n",
            "[4579 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On prépare les données pour les fournir au lstm (reshaping+scaling+conversion en flottants et en tableaux numpy)"
      ],
      "metadata": {
        "id": "ieTneJcsZKGL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV_v5jR2CcZu",
        "outputId": "513e06b6-bb20-4665-8115-c10a94a3fae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (4489, 90, 1), X dtype: float32\n",
            "y shape: (4489, 2), y dtype: float32\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#\"Vraie\" volatilité issue de l'estimation de la moyenne par le modèle garch : (r_t-mu_t)**2\n",
        "true_volatility = predictions_df['True_Volatility'].values\n",
        "\n",
        "# volatilité prédite issue du modèle garch\n",
        "predicted_volatility = predictions_df['Predicted_Volatility'].values\n",
        "\n",
        "# Preparer la data pour le LSTM\n",
        "X = np.array([true_volatility[i-90:i] for i in range(90, len(true_volatility))], dtype=np.float32).reshape(-1, 90, 1)\n",
        "y = np.vstack((true_volatility[90:], predicted_volatility[90:])).astype(np.float32).T  # Shape: [n_samples, 2]\n",
        "\n",
        "# Standardize X pour de meilleures perfs\n",
        "scaler_X = StandardScaler()\n",
        "X_scaled = np.array([scaler_X.fit_transform(seq.reshape(-1, 1)) for seq in X], dtype=np.float32)  # Reshape each sequence to 2D before scaling\n",
        "\n",
        "# Standardize y pour de meilleures perfs\n",
        "scaler_y = StandardScaler()\n",
        "y_scaled = scaler_y.fit_transform(y)  # Standardize y\n",
        "\n",
        "# Check\n",
        "print(f\"X shape: {X_scaled.shape}, X dtype: {X_scaled.dtype}\")\n",
        "print(f\"y shape: {y_scaled.shape}, y dtype: {y_scaled.dtype}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On définit la fonction cout customisée que l'on va utiliser ici:"
      ],
      "metadata": {
        "id": "siHDl6e6Zsy_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPRhMXADVvJs",
        "outputId": "cc58dd03-642b-4466-be8a-e36f9e828446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - loss: 0.0921\n",
            "Epoch 2/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0221\n",
            "Epoch 3/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0219\n",
            "Epoch 4/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 0.0187\n",
            "Epoch 5/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 0.0207\n",
            "Epoch 6/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 0.0188\n",
            "Epoch 7/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 0.0187\n",
            "Epoch 8/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0189\n",
            "Epoch 9/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0181\n",
            "Epoch 10/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0188\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f5bb81d4e50>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Fonction coût utilisée lors de l'entraînement:\n",
        "\n",
        "def custom_loss(predicted_volatility, lambda_param=0.5):\n",
        "    predicted_volatility = tf.convert_to_tensor(predicted_volatility, dtype=tf.float32)\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        batch_size = tf.shape(y_true)[0]\n",
        "        predicted_vol_batch = predicted_volatility[:batch_size]\n",
        "\n",
        "        mse = tf.keras.losses.MeanSquaredError()\n",
        "        mse_true = mse(y_true, y_pred)\n",
        "        mse_garch = mse(predicted_vol_batch, y_pred)\n",
        "\n",
        "        return lambda_param * mse_true + (1 - lambda_param) * mse_garch\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Initialisation et Compilation\n",
        "model = create_lstm_model()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005, clipnorm=1.0)  # Clip gradients with L2 norm threshold = 1.0\n",
        "model.compile(optimizer=optimizer, loss=custom_loss(y_scaled[:,1],lambda_param=0.01))\n",
        "\n",
        "# Entrainement\n",
        "model.fit(X_scaled, y_scaled[:, 0], epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On prédit les volatilités par le lstm"
      ],
      "metadata": {
        "id": "4rZiNxOqaKOx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiT4tkJ8Fdhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8a7d5f-403b-4f48-8c50-fb5d041ec87c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
            "[[-0.4587227 ]\n",
            " [-0.45028377]\n",
            " [-0.44378483]\n",
            " ...\n",
            " [-0.3782543 ]\n",
            " [-0.37854367]\n",
            " [-0.37898362]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "y_pred_scaled = model.predict(X_scaled)  # Shape: (n_samples, 1)\n",
        "print(y_pred_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On enlève le scaling sur nos prédictions"
      ],
      "metadata": {
        "id": "c7_rBZGGaQLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse transform du scaling vers le scale original\n",
        "\n",
        "\n",
        "dummy_array = np.zeros_like(y_scaled)\n",
        "dummy_array[:, 1] = y_pred_scaled.flatten()  # Replace the second column with predicted volatilities\n",
        "\n",
        "# Inverse transform\n",
        "y_pred_inverse = scaler_y.inverse_transform(dummy_array)  # Shape: (n_samples, 2)"
      ],
      "metadata": {
        "id": "LV5PZbHOXimY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On affiche les résultats avec les log_returns issus de la prédiction"
      ],
      "metadata": {
        "id": "90X-MEAZahRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCqCsPVlAzNo",
        "outputId": "13f017f3-eb6e-49c0-fb9e-d3c10e6be4e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date  Predicted_Mean  Predicted_Volatility GINN  Log_Return\n",
            "0    1994-09-20        0.093853                   0.358098   -0.367785\n",
            "1    1994-09-21        0.077710                   0.376151   -0.369584\n",
            "2    1994-09-22        0.074503                   0.390054   -0.358040\n",
            "3    1994-09-23        0.057090                   0.397799   -0.374252\n",
            "4    1994-09-26        0.049137                   0.395731   -0.388300\n",
            "...         ...             ...                        ...         ...\n",
            "4484 2012-07-10       -0.020004                   0.531452   -0.343894\n",
            "4485 2012-07-11       -0.025942                   0.530790   -0.352950\n",
            "4486 2012-07-12       -0.028566                   0.530238   -0.357234\n",
            "4487 2012-07-13       -0.003205                   0.529619   -0.322212\n",
            "4488 2012-07-16        0.000739                   0.528678   -0.317672\n",
            "\n",
            "[4489 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "predicted_volatility_lstm_original_scale = y_pred_inverse[:, 1]  # Shape: (n_samples,)\n",
        "\n",
        "# Step 3: Combiner predicted volatilities par le LSTM avec les moyennes correspodnantes\n",
        "means = predictions_df['Predicted_Mean'].values[90:]  # Shape: (n_samples,)\n",
        "\n",
        "# Calculer log_returns issus de la prédiction par le LSTM\n",
        "log_returns_pred = means + np.sqrt(predicted_volatility_lstm_original_scale)\n",
        "\n",
        "output_df = pd.DataFrame({\n",
        "    'Date': predictions_df.index[90:],  # Use the corresponding dates\n",
        "    'Predicted_Mean': means,\n",
        "    'Predicted_Volatility GINN': predicted_volatility_lstm_original_scale,\n",
        "    'Log_Return': log_returns_pred\n",
        "})\n",
        "\n",
        "# output DataFrame\n",
        "print(output_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocoDYglgF6Mm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fd3c1f-b34b-45c2-bf2a-dbc56b434381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.2361291e-02 6.7635365e-03 6.1256648e-03 ... 6.8031077e-04 3.7192210e-04\n",
            " 2.1625070e-05]\n"
          ]
        }
      ],
      "source": [
        "print(y[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_volatility)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8N6meKGW2h5",
        "outputId": "7b0308f3-4127-43f6-829f-d0977b19041b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.56489893 0.54862113 0.49840484 ... 0.89146804 0.94036626 0.98801149]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_log_returns(dji_close.iloc[180:,:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "GPzrFmjjRXiN",
        "outputId": "86244fb6-a1d9-42a7-da02-72ba9ca4fd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'calculate_log_returns' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ce5fcb8f33fd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_log_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdji_close\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'calculate_log_returns' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CTWaE2tLNNbt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}